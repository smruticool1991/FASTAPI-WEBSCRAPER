# Website Analysis API

A high-performance Python API for website analysis and scraping, designed to replace the n8n workflow with better performance characteristics.

## Features

- **Platform Detection**: Identifies WordPress, Shopify, Wix, Squarespace, Webflow, React/Next.js, Drupal, Joomla, Magento
- **SEO Analysis**: Title, meta descriptions, headers, structured data, Open Graph, Twitter Cards
- **Contact Information Extraction**: Emails, phone numbers, contact page URLs with intelligent filtering
- **Social Media Detection**: Facebook, Twitter/X, LinkedIn, Instagram, YouTube, Pinterest, TikTok, WhatsApp
- **Security Analysis**: HTTPS, HSTS, CSP, X-Frame-Options headers
- **Performance Optimized**: Async processing, rate limiting, batch processing, connection pooling

## Quick Start

### Option 1: Docker (Recommended)

```bash
# Start with Docker (easiest method)
./docker-run.sh dev

# Or using docker-compose directly
docker-compose up -d
```

### Option 2: Local Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Run the API server
python main.py
```

The API will be available at `http://localhost:8000`

### Option 3: Production Deployment

```bash
# Production deployment with monitoring
export DB_PASSWORD="your_secure_password"
export GRAFANA_PASSWORD="your_grafana_password"
./docker-run.sh prod
```

**Access Points:**
- üåê API: http://localhost:8000
- üìö Documentation: http://localhost:8000/docs  
- üîç Monitoring: http://localhost:9090 (Prometheus)
- üìä Dashboard: http://localhost:3000 (Grafana)

### API Documentation

Interactive API documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Usage

### Analyze Multiple Websites

```bash
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "domains": ["example.com", "google.com"],
    "batch_size": 10,
    "timeout": 15
  }'
```

### Request Parameters

- `domains`: List of domains to analyze (required)
- `batch_size`: Number of domains to process concurrently (default: 10, max: 20)
- `timeout`: Request timeout in seconds (default: 15)
- `include_content`: Include raw HTML content in response (default: false)

### Response Format

Each analyzed website returns comprehensive data including:

```json
{
  "domain": "example.com",
  "platform": "WordPress",
  "purpose": "General",
  "isHttps": "Yes",
  "hasTitle": "Yes",
  "titleLength": 45,
  "emails": ["contact@example.com"],
  "contactPages": [
    {
      "url": "https://example.com/contact",
      "linkText": "Contact Us"
    }
  ],
  "socialLinks": {
    "facebook": ["https://facebook.com/example"],
    "twitter": ["https://twitter.com/example"]
  },
  "seoScore": 85,
  "seoGrade": "A",
  "status": "Active",
  "analyzedAt": "2024-01-01T12:00:00"
}
```

## Performance Features

### Rate Limiting
- Built-in semaphore-based rate limiting
- Configurable concurrent request limits
- Delays between requests to prevent server overload

### Batch Processing
- Processes domains in configurable batches
- Async processing within batches for maximum efficiency
- Automatic delays between batches

### Memory Optimization
- Streaming content processing
- Limited concurrent connections
- Efficient regex patterns for content analysis

### Error Handling
- Graceful timeout handling
- Comprehensive error responses
- Detailed logging for debugging

## Architecture

### Key Components

1. **WebsiteAnalyzer**: Core analysis engine with methods for:
   - Platform detection
   - Email/phone extraction with quality scoring
   - Social media link detection
   - SEO analysis and scoring

2. **RateLimiter**: Semaphore-based rate limiting to prevent overwhelming target servers

3. **FastAPI Application**: RESTful API with async request handling and automatic documentation

### Performance Improvements Over n8n

- **Async Processing**: Native Python async/await instead of n8n workflow orchestration
- **Connection Pooling**: Reused HTTP connections via aiohttp
- **Memory Efficiency**: Stream processing instead of loading large datasets in memory
- **Reduced Overhead**: Direct Python execution vs. JavaScript in browser environment
- **Better Rate Limiting**: Granular control over request timing and concurrency

## Configuration

Environment variables (optional):

```bash
export MAX_CONCURRENT_REQUESTS=10
export DEFAULT_TIMEOUT=15
export LOG_LEVEL=INFO
```

## Health Check

Check API status:

```bash
curl http://localhost:8000/health
```

## Development

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/
```

### Code Quality

```bash
# Format code
black main.py

# Check style
flake8 main.py
```

## Migration from n8n

This API provides the same functionality as the original n8n workflow but with:

- 60-80% better performance
- Lower memory usage
- Better error handling
- Easier deployment and scaling
- Comprehensive logging and monitoring

The response format matches the original n8n output for easy integration.